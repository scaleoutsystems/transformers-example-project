{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS NOTEBOOK IS BASED ON THE ORIGINAL NOTEBOOK PUBLISHED BY AF-AI at https://github.com/af-ai-center/SweBERT.git\n",
    "\n",
    "We have modified the prediction example at the end to use Tensorflow instead of PyTorch. \n",
    "\n",
    "To run in STACKn starting from the default image, you need to (from a Terminal session) \n",
    "\n",
    "    $ pip install -r requirements.txt\n",
    "    $ jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "# Content:\n",
    "1. Check SweBERT Model Accessibility\n",
    "2. Simple Model Application (Masked Token Prediction)\n",
    "\n",
    "#### Note: Make sure to run this notebook in a virtual environment with the required packages (see README) installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, BertModel, TFBertModel, BertForMaskedLM \n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Choose SweBERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to choose one of the pretrained SweBERT models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = 'af-ai-center/bert-base-swedish-uncased'\n",
    "# pretrained_model_name = af-ai-center/bert-large-swedish-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Check SweBERT Model Accessibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to check that the chosen pretrained SweBERT model is accessible through the transformers library.\n",
    "If it is, we should be able to instantiate a tokenizer and a (PyTorch/TensorFlow) model from it. \n",
    "\n",
    "Note that this may take a while the first time you run it as the model needs to be downloaded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73fc2efe67a44225a0b35ebfb3de8f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=254112.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_name, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Model PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905032a92dfa4e29841e464562c55fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5988e5032f6646d398f9572923e377c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440474402.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Model TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f84b4143fcc4beabbb589118617ddd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=438190872.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFBertModel.\n",
      "\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at af-ai-center/bert-base-swedish-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertModel.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Simple Model Application (Masked Token Prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to apply the (PyTorch) SweBERT model on an example sentence, loosely following https://huggingface.co/transformers/quickstart.html#quick-tour-usage\n",
    "\n",
    "We will\n",
    "1. Tokenize the example using BertTokenizer\n",
    "2. Tokenize the example using BertWordPieceTokenizer\n",
    "3. Mask one of the tokens\n",
    "4. Use SweBERT to predict back the masked token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jag är ett barn, och det här är mitt hem. Alltså är det ett barnhem!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = 'Jag är ett barn, och det här är mitt hem. Alltså är det ett barnhem!'\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenize the example using BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained SweBERT models are uncased. \n",
    "\n",
    "In principle, we could account for this by instantiating the BertTokenizer (https://huggingface.co/transformers/model_doc/bert.html#berttokenizer) with the parameter `do_lower_case=True`.\n",
    "However, the BertTokenizer does not handle the Swedish letters `å, ä & ö` properly (they get replaced by `a & o`).\n",
    "\n",
    "To avoid this problem, we manually lowercase all text before tokenization instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(pretrained_model_name, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. lowercase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jag är ett barn, och det här är mitt hem. alltså är det ett barnhem!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_uncased = example.lower()\n",
    "example_uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. add special tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of BERT models needs to be provided with special tokens '[CLS]' and '[SEP]':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] jag är ett barn, och det här är mitt hem. alltså är det ett barnhem! [SEP]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_preprocessed = f'[CLS] {example_uncased} [SEP]'\n",
    "example_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 tokens:\n",
      "['[CLS]', 'jag', 'är', 'ett', 'barn', ',', 'och', 'det', 'här', 'är', 'mitt', 'hem', '.', 'alltså', 'är', 'det', 'ett', 'barn', '##hem', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = bert_tokenizer.tokenize(example_preprocessed)\n",
    "\n",
    "print(f'{len(tokens)} tokens:')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. convert tokens to ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1112, 1100, 1115, 1255, 1010, 1095, 1102, 1174, 1100, 1352, 1345, 1012, 1492, 1100, 1102, 1115, 1255, 2760, 999, 102]\n"
     ]
    }
   ],
   "source": [
    "indexed_tokens = bert_tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(indexed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenize the example using BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative is to use the BertWordPieceTokenizer from the tokenizers library (https://github.com/huggingface/tokenizers).\n",
    "It handles the special Swedish letters properly if the parameters `lowercase=True` & `strip_accents=False` are used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_word_piece_tokenizer = BertWordPieceTokenizer(\"vocab_swebert.txt\", lowercase=True, strip_accents=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. tokenize & d. convert tokens to ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = bert_word_piece_tokenizer.encode(example)  # attributes: output.ids, output.tokens, output.offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 tokens:\n",
      "['[CLS]', 'jag', 'är', 'ett', 'barn', ',', 'och', 'det', 'här', 'är', 'mitt', 'hem', '.', 'alltså', 'är', 'det', 'ett', 'barn', '##hem', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens_2 = output.tokens\n",
    "\n",
    "print(f'{len(tokens_2)} tokens:')\n",
    "print(tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1112, 1100, 1115, 1255, 1010, 1095, 1102, 1174, 1100, 1352, 1345, 1012, 1492, 1100, 1102, 1115, 1255, 2760, 999, 102]\n"
     ]
    }
   ],
   "source": [
    "indexed_tokens_2 = output.ids\n",
    "print(indexed_tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that BertTokenizer & BertWordPieceTokenizer lead to the same results\n",
    "assert tokens == tokens_2\n",
    "assert indexed_tokens == indexed_tokens_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Mask one of the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_index = 17  # 'barn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'jag', 'är', 'ett', 'barn', ',', 'och', 'det', 'här', 'är', 'mitt', 'hem', '.', 'alltså', 'är', 'det', 'ett', '[MASK]', '##hem', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens[masked_index] = '[MASK]'\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1112, 1100, 1115, 1255, 1010, 1095, 1102, 1174, 1100, 1352, 1345, 1012, 1492, 1100, 1102, 1115, 103, 2760, 999, 102]\n"
     ]
    }
   ],
   "source": [
    "# Mask token with BertTokenizer\n",
    "indexed_tokens[masked_index] = bert_tokenizer.convert_tokens_to_ids('[MASK]')\n",
    "print(indexed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1112, 1100, 1115, 1255, 1010, 1095, 1102, 1174, 1100, 1352, 1345, 1012, 1492, 1100, 1102, 1115, 103, 2760, 999, 102]\n"
     ]
    }
   ],
   "source": [
    "# Mask token with BertWordPieceTokenizer\n",
    "indexed_tokens[masked_index] = bert_word_piece_tokenizer.token_to_id('[MASK]')\n",
    "print(indexed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use SweBERT to predict back the masked token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at af-ai-center/bert-base-swedish-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at af-ai-center/bert-base-swedish-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# instantiate model\n",
    "model = BertForMaskedLM.from_pretrained(pretrained_model_name)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 21, 30522])\n"
     ]
    }
   ],
   "source": [
    "# predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(torch.tensor([indexed_tokens]))\n",
    "\n",
    "predictions = outputs[0]\n",
    "print(predictions.shape)  # 1 example, 21 tokens, 30522 vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1255)\n"
     ]
    }
   ],
   "source": [
    "# show prediction for masked token's index\n",
    "predicted_index = torch.argmax(predictions[0, masked_index])\n",
    "print(predicted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barn\n"
     ]
    }
   ],
   "source": [
    "# show prediction for masked token\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "print(predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert predicted_token == 'barn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of looking only at the top model prediction, we can also consider the top 5 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1255,  8032, 14829,  1251,  1264])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show top5 predictions for masked token's index\n",
    "predicted_index_top5 = torch.argsort(predictions[0, masked_index], descending=True)[:5]\n",
    "predicted_index_top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barn\n",
      "foster\n",
      "barndoms\n",
      "dock\n",
      "dag\n"
     ]
    }
   ],
   "source": [
    "# show top5 predictions for masked token\n",
    "for predicted_index in predicted_index_top5:\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "    print(predicted_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have checked the accessibility of the SweBERT models through the transformers library. \n",
    "- We have demonstrated a very simple model application, where the SweBERT model successfully predicts a masked token.\n",
    "\n",
    "For additional use cases and information, we refer to the documentation of the transformers library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
